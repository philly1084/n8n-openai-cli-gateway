apiVersion: v1
kind: Namespace
metadata:
  name: n8n-openai-gateway
  labels:
    app.kubernetes.io/name: n8n-openai-cli-gateway
---
apiVersion: v1
kind: Secret
metadata:
  name: n8n-openai-cli-gateway-secrets
  namespace: n8n-openai-gateway
type: Opaque
stringData:
  n8nApiKey: "replace-with-long-random-n8n-key"
  adminApiKey: "replace-with-long-random-admin-key"
---
apiVersion: v1
kind: Secret
metadata:
  name: ghcr-secret
  namespace: n8n-openai-gateway
type: kubernetes.io/dockerconfigjson
stringData:
  .dockerconfigjson: |
    {
      "auths": {
        "ghcr.io": {
          "username": "replace-with-github-username",
          "password": "replace-with-github-token-read-packages"
        }
      }
    }
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
imagePullSecrets:
  - name: ghcr-secret
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: n8n-openai-cli-gateway-config
  namespace: n8n-openai-gateway
data:
  providers.yaml: |
    providers:
      - id: ollama-cli
        type: cli
        description: Ollama local adapter (GLM4 Q4 only)
        models:
          - id: ollama-glm4-q4
            providerModel: glm4:9b-chat-q4_K_M
        responseCommand:
          executable: sh
          args:
            - -lc
            - |
              REQUEST_JSON="$(cat)"
              OLLAMA_URL="${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}"
              OLLAMA_NUM_THREAD="${OLLAMA_NUM_THREAD:-6}"
              OLLAMA_NUM_CTX="${OLLAMA_NUM_CTX:-2048}"
              OLLAMA_NUM_PREDICT="${OLLAMA_NUM_PREDICT:-384}"
              HAS_TOOLS="$(
                printf "%s" "$REQUEST_JSON" | node -e 'let d="";process.stdin.on("data",c=>d+=c).on("end",()=>{try{const req=JSON.parse(d);const tools=Array.isArray(req.tools)?req.tools:[];process.stdout.write(tools.length>0?"1":"0");}catch{process.stdout.write("0");}});'
              )"

              PAYLOAD="$(
                printf "%s" "$REQUEST_JSON" | node -e 'let d="";const numThread=Number(process.argv[1]);const numCtx=Number(process.argv[2]);const numPredict=Number(process.argv[3]);process.stdin.on("data",c=>d+=c).on("end",()=>{const req=JSON.parse(d);const msgs=Array.isArray(req.messages)?req.messages:[];const tools=Array.isArray(req.tools)?req.tools:[];const normalized=msgs.map((m)=>{const out={role:String(m.role||"user"),content:typeof m.content==="string"?m.content:JSON.stringify(m.content??"")};if(out.role==="tool"){const n=typeof m.name==="string"&&m.name?m.name:(typeof m.tool_call_id==="string"&&m.tool_call_id?m.tool_call_id:undefined);if(n)out.tool_name=n;}return out;});const body={model:"{{provider_model}}",messages:normalized,tools,stream:false};const options={};if(Number.isFinite(numThread)&&numThread>0){options.num_thread=Math.trunc(numThread);}if(Number.isFinite(numCtx)&&numCtx>0){options.num_ctx=Math.trunc(numCtx);}if(Number.isFinite(numPredict)&&numPredict>0){options.num_predict=Math.trunc(numPredict);}if(Object.keys(options).length>0){body.options=options;}process.stdout.write(JSON.stringify(body));});' "$OLLAMA_NUM_THREAD" "$OLLAMA_NUM_CTX" "$OLLAMA_NUM_PREDICT"
              )"

              RESPONSE_JSON="$(curl -fsS "$OLLAMA_URL/api/chat" -H "Content-Type: application/json" -d "$PAYLOAD")" || exit $?

              printf "%s" "$RESPONSE_JSON" | node -e '
                let d="";
                const hasTools=process.argv[1]==="1";
                const providerModel=String(process.argv[2]||"").toLowerCase();
                const isPhiModel=/\bphi/.test(providerModel);
                const looksLikePretendToolUse=(value)=>{
                  if(typeof value!=="string") return false;
                  const t=value.toLowerCase();
                  if(t.includes("simulated action")) return true;
                  if(t.includes("working memory tool")) return true;
                  if(t.includes("status table")) return true;
                  if(t.includes("query memory")) return true;
                  if(/\b(use|using|call|invoke|query|save|store|write)\b[\s\S]{0,100}\b(tool|memory|table)\b/.test(t)) return true;
                  return false;
                };
                const maybeRejectMissingToolCalls=(text,toolCallCount)=>{
                  if(!hasTools||!isPhiModel||toolCallCount>0) return false;
                  if(!looksLikePretendToolUse(text)) return false;
                  process.stderr.write("ollama phi pseudo-tool output detected without tool_calls\n");
                  return true;
                };
                process.stdin.on("data",c=>d+=c).on("end",()=>{
                  const res=JSON.parse(d);
                  const msg=res&&typeof res.message==="object"&&res.message?res.message:{};
                  const content=typeof msg.content==="string"?msg.content:"";

                  const normalizeToolCalls=(arr)=>{
                    const calls=Array.isArray(arr)?arr:[];
                    return calls.map((c,i)=>{
                      if(!c||typeof c!=="object") return null;
                      const fn=c.function&&typeof c.function==="object"?c.function:{};
                      const name=typeof c.name==="string"&&c.name
                        ?c.name
                        :(typeof fn.name==="string"&&fn.name?fn.name:"");
                      if(!name) return null;
                      const args=c.arguments!==undefined
                        ?c.arguments
                        :(fn.arguments!==undefined?fn.arguments:{});
                      return {
                        id:typeof c.id==="string"&&c.id?c.id:`call_${i+1}`,
                        name,
                        arguments:typeof args==="string"?args:JSON.stringify(args)
                      };
                    }).filter(Boolean);
                  };

                  const rawToolCalls=normalizeToolCalls(msg.tool_calls);
                  if(rawToolCalls.length>0){
                    process.stdout.write(JSON.stringify({
                      output_text:content,
                      tool_calls:rawToolCalls,
                      finish_reason:"tool_calls"
                    }));
                    return;
                  }

                  const parseContract=(input)=>{
                    const t=typeof input==="string"?input.trim():"";
                    if(!t) return null;
                    const seen=new Set();
                    const queue=[];
                    const push=(s)=>{
                      if(typeof s!=="string") return;
                      const v=s.trim();
                      if(!v||seen.has(v)) return;
                      seen.add(v);
                      queue.push(v);
                    };
                    const pushDerived=(s)=>{
                      const fence=/```(?:json)?\s*([\s\S]*?)```/gi;
                      let m;
                      while((m=fence.exec(s))!==null){
                        push(m[1]);
                      }
                      const start=s.indexOf("{");
                      const end=s.lastIndexOf("}");
                      if(start!==-1&&end>start){
                        push(s.slice(start,end+1));
                      }
                    };
                    const isContract=(j)=>j&&typeof j==="object"&&(
                      "output_text" in j || "tool_calls" in j || "finish_reason" in j || "text" in j || "content" in j
                    );

                    push(t);
                    pushDerived(t);

                    for(let i=0;i<queue.length&&i<50;i++){
                      const cur=queue[i];
                      pushDerived(cur);
                      let parsed=null;
                      try{parsed=JSON.parse(cur);}catch{}
                      if(!parsed||typeof parsed!=="object") continue;

                      if(isContract(parsed)){
                        const innerCandidates=[
                          typeof parsed.output_text==="string"?parsed.output_text:"",
                          typeof parsed.text==="string"?parsed.text:"",
                          typeof parsed.content==="string"?parsed.content:""
                        ].filter(Boolean);
                        for(const c of innerCandidates){
                          push(c);
                          pushDerived(c);
                          try{
                            const j2=JSON.parse(c);
                            if(isContract(j2)) return j2;
                          }catch{}
                        }
                        return parsed;
                      }

                      if(typeof parsed.response==="string") push(parsed.response);
                      if(parsed.message&&typeof parsed.message==="object"&&typeof parsed.message.content==="string"){
                        push(parsed.message.content);
                      }
                    }

                    return null;
                  };

                  const parsedContract=parseContract(content);
                  if(parsedContract){
                    const parsedToolCalls=normalizeToolCalls(parsedContract.tool_calls);
                    const outputText=typeof parsedContract.output_text==="string"
                      ?parsedContract.output_text
                      :(typeof parsedContract.text==="string"
                        ?parsedContract.text
                        :(typeof parsedContract.content==="string"?parsedContract.content:content));
                    if(maybeRejectMissingToolCalls(outputText,parsedToolCalls.length)){
                      process.exit(86);
                    }
                    const finishReason=parsedToolCalls.length>0
                      ?"tool_calls"
                      :(typeof parsedContract.finish_reason==="string"&&parsedContract.finish_reason
                        ?parsedContract.finish_reason
                        :"stop");
                    process.stdout.write(JSON.stringify({
                      output_text:outputText,
                      tool_calls:parsedToolCalls,
                      finish_reason:finishReason
                    }));
                    return;
                  }

                  const unfenced=content
                    .replace(/^```(?:json)?\s*/i,"")
                    .replace(/```$/,"")
                    .trim();
                  if(maybeRejectMissingToolCalls(unfenced||content,0)){
                    process.exit(86);
                  }
                  process.stdout.write(JSON.stringify({
                    output_text:unfenced||content,
                    finish_reason:"stop"
                  }));
                });
              ' "$HAS_TOOLS" "{{provider_model}}"
          input: request_json_stdin
          output: json_contract
          timeoutMs: 240000
        auth:
          loginCommand:
            executable: sh
            args:
              - -lc
              - |
                echo "Ollama local provider does not require login."
                curl -fsS "${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}/api/tags" >/dev/null
            timeoutMs: 30000
          statusCommand:
            executable: sh
            args:
              - -lc
              - |
                curl -fsS "${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}/api/tags"
            timeoutMs: 30000
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: n8n-openai-cli-gateway-home
  namespace: n8n-openai-gateway
spec:
  # Set this if your cluster does not have a default storage class.
  # storageClassName: replace-with-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-models
  namespace: n8n-openai-gateway
spec:
  # Set this if your cluster does not have a default storage class.
  # storageClassName: replace-with-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: n8n-openai-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      nodeSelector:
        kubernetes.io/arch: arm64
      containers:
        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 11434
              name: http
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0:11434"
            - name: OLLAMA_CONTEXT_LENGTH
              value: "2048"
          volumeMounts:
            - name: ollama-models
              mountPath: /root/.ollama
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 20
          resources:
            requests:
              cpu: "2"
              memory: 8Gi
            limits:
              cpu: "10"
              memory: 16Gi
      volumes:
        - name: ollama-models
          persistentVolumeClaim:
            claimName: ollama-models
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: n8n-openai-gateway
  labels:
    app: ollama
spec:
  type: ClusterIP
  selector:
    app: ollama
  ports:
    - name: http
      port: 11434
      targetPort: 11434
      protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: n8n-openai-cli-gateway
  template:
    metadata:
      labels:
        app: n8n-openai-cli-gateway
    spec:
      serviceAccountName: n8n-openai-cli-gateway
      nodeSelector:
        kubernetes.io/arch: arm64
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        fsGroup: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: gateway
          image: ghcr.io/philly1084/n8n-openai-cli-gateway:main
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            capabilities:
              drop: ["ALL"]
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: PORT
              value: "8080"
            - name: HOST
              value: "0.0.0.0"
            - name: LOG_LEVEL
              value: "info"
            - name: MAX_JOB_LOG_LINES
              value: "500"
            - name: PROVIDERS_CONFIG_PATH
              value: /app/config/providers.yaml
            - name: HOME
              value: /var/lib/gateway-home
            - name: OLLAMA_BASE_URL
              value: "http://ollama.n8n-openai-gateway.svc.cluster.local:11434"
            - name: OLLAMA_NUM_THREAD
              value: "6"
            - name: OLLAMA_NUM_CTX
              value: "2048"
            - name: OLLAMA_NUM_PREDICT
              value: "384"
            - name: N8N_API_KEY
              valueFrom:
                secretKeyRef:
                  name: n8n-openai-cli-gateway-secrets
                  key: n8nApiKey
            - name: ADMIN_API_KEY
              valueFrom:
                secretKeyRef:
                  name: n8n-openai-cli-gateway-secrets
                  key: adminApiKey
          volumeMounts:
            - name: providers-config
              mountPath: /app/config/providers.yaml
              subPath: providers.yaml
            - name: provider-home
              mountPath: /var/lib/gateway-home
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 15
          startupProbe:
            httpGet:
              path: /healthz
              port: 8080
            failureThreshold: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: "1"
              memory: 1Gi
      volumes:
        - name: providers-config
          configMap:
            name: n8n-openai-cli-gateway-config
        - name: provider-home
          persistentVolumeClaim:
            claimName: n8n-openai-cli-gateway-home
---
apiVersion: v1
kind: Service
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
  labels:
    app: n8n-openai-cli-gateway
spec:
  type: ClusterIP
  selector:
    app: n8n-openai-cli-gateway
  ports:
    - name: http
      port: 80
      targetPort: 8080
      protocol: TCP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - gateway.example.com
      secretName: n8n-openai-cli-gateway-tls
  rules:
    - host: gateway.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: n8n-openai-cli-gateway
                port:
                  number: 80
