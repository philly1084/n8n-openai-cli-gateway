apiVersion: v1
kind: Namespace
metadata:
  name: n8n-openai-gateway
  labels:
    app.kubernetes.io/name: n8n-openai-cli-gateway
---
apiVersion: v1
kind: Secret
metadata:
  name: n8n-openai-cli-gateway-secrets
  namespace: n8n-openai-gateway
type: Opaque
stringData:
  n8nApiKey: "replace-with-long-random-n8n-key"
  adminApiKey: "replace-with-long-random-admin-key"
---
apiVersion: v1
kind: Secret
metadata:
  name: ghcr-secret
  namespace: n8n-openai-gateway
type: kubernetes.io/dockerconfigjson
stringData:
  .dockerconfigjson: |
    {
      "auths": {
        "ghcr.io": {
          "username": "replace-with-github-username",
          "password": "replace-with-github-token-read-packages"
        }
      }
    }
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
imagePullSecrets:
  - name: ghcr-secret
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: n8n-openai-cli-gateway-config
  namespace: n8n-openai-gateway
data:
  providers.yaml: |
    providers:
      - id: gemini-cli
        type: cli
        description: Gemini CLI adapter
        models:
          - id: gemini-3-pro-preview
            providerModel: gemini-3-pro-preview
            fallbackModels:
              - gpt-5-codex
              - gpt-4.1
          - id: gemini-3-flash-preview
            providerModel: gemini-3-flash-preview
            fallbackModels:
              - gpt-4.1
              - gpt-5-codex
          - id: gemini-2.5-pro
            providerModel: gemini-2.5-pro
            fallbackModels:
              - gpt-4.1
              - gpt-5-codex
          - id: gemini-2.5-flash
            providerModel: gemini-2.5-flash
            fallbackModels:
              - gpt-4.1
              - gpt-5-codex
          - id: gemini-2.5-flash-lite
            providerModel: gemini-2.5-flash-lite
            fallbackModels:
              - gpt-4.1
              - gpt-5-codex
        responseCommand:
          executable: sh
          args:
            - -lc
            - |
              REQUEST_JSON="$(cat)"
              PROMPT="$(
                printf "%s" "$REQUEST_JSON" | node -e 'let d="";process.stdin.on("data",c=>d+=c).on("end",()=>{const req=JSON.parse(d);const msgs=Array.isArray(req.messages)?req.messages:[];const tools=Array.isArray(req.tools)?req.tools:[];const norm=v=>typeof v==="string"?v:JSON.stringify(v??"");const msgText=msgs.map(m=>`${String(m.role||"user").toUpperCase()}:\n${norm(m.content)}`).join("\n\n");const toolJson=JSON.stringify(tools,null,2);const instruction=["You are connected through an OpenAI-compatible gateway.","Do not use any internal tools, shell commands, filesystem access, web browsing, or MCP tools.","Return raw JSON only. Do not wrap JSON in markdown/code fences.","If you need to call a tool, respond ONLY with JSON:","{\"output_text\":\"\",\"tool_calls\":[{\"id\":\"call_1\",\"name\":\"tool_name\",\"arguments\":{\"arg\":\"value\"}}],\"finish_reason\":\"tool_calls\"}","If no tool is needed, respond ONLY with JSON:","{\"output_text\":\"<assistant reply>\",\"finish_reason\":\"stop\"}"].join("\n");process.stdout.write([msgText,"","AVAILABLE_TOOLS_JSON:",toolJson,"",instruction].join("\n"));});'
              )"
              RAW_OUTPUT="$(gemini --model "{{provider_model}}" --output-format text --prompt "$PROMPT")" || exit $?
              printf "%s" "$RAW_OUTPUT" | node -e '
                let d="";
                process.stdin.on("data",c=>d+=c).on("end",()=>{
                  const t=d.trim();
                  if(!t){
                    process.stdout.write(JSON.stringify({output_text:"",finish_reason:"stop"}));
                    return;
                  }

                  const seen=new Set();
                  const queue=[];
                  const push=(s)=>{
                    if(typeof s!=="string") return;
                    const v=s.trim();
                    if(!v||seen.has(v)) return;
                    seen.add(v);
                    queue.push(v);
                  };

                  const pushDerived=(s)=>{
                    const fence=/```(?:json)?\s*([\s\S]*?)```/gi;
                    let m;
                    while((m=fence.exec(s))!==null){
                      push(m[1]);
                    }
                    const start=s.indexOf("{");
                    const end=s.lastIndexOf("}");
                    if(start!==-1&&end>start){
                      push(s.slice(start,end+1));
                    }
                  };

                  const isContract=(j)=>j&&typeof j==="object"&&(
                    "output_text" in j || "tool_calls" in j || "finish_reason" in j || "text" in j || "content" in j
                  );

                  push(t);
                  pushDerived(t);

                  for(let i=0;i<queue.length&&i<50;i++){
                    const cur=queue[i];
                    pushDerived(cur);
                    let parsed=null;
                    try{ parsed=JSON.parse(cur);}catch{}
                    if(!parsed||typeof parsed!=="object") continue;

                    if(isContract(parsed)){
                      const innerCandidates=[
                        typeof parsed.output_text==="string"?parsed.output_text:"",
                        typeof parsed.text==="string"?parsed.text:"",
                        typeof parsed.content==="string"?parsed.content:""
                      ].filter(Boolean);

                      let promoted=null;
                      for(const c of innerCandidates){
                        push(c);
                        pushDerived(c);
                        try{
                          const j2=JSON.parse(c);
                          if(isContract(j2)){promoted=j2;break;}
                        }catch{}
                      }
                      process.stdout.write(JSON.stringify(promoted||parsed));
                      return;
                    }

                    if(typeof parsed.response==="string") push(parsed.response);
                    if(parsed.message&&typeof parsed.message==="object"&&typeof parsed.message.content==="string"){
                      push(parsed.message.content);
                    }
                  }

                  const unfenced=t
                    .replace(/^```(?:json)?\s*/i,"")
                    .replace(/```$/,"")
                    .trim();
                  process.stdout.write(JSON.stringify({output_text:unfenced||t,finish_reason:"stop"}));
                });
              '
          input: request_json_stdin
          output: json_contract
          timeoutMs: 240000
        auth:
          loginCommand:
            executable: sh
            args:
              - -c
              - |
                echo "Gemini CLI login is interactive in this version."
                echo "Run in a TTY shell: gemini"
                echo "Then run: /auth and choose Login with Google."
                exit 1
            timeoutMs: 10000
          statusCommand:
            executable: sh
            args:
              - -c
              - |
                test -f "$HOME/.gemini/oauth_creds.json" \
                  && echo '{"ok":true,"source":"oauth_creds.json"}' \
                  || (echo '{"ok":false,"error":"missing ~/.gemini/oauth_creds.json"}' && exit 1)
            timeoutMs: 30000
      - id: antigravity-cli
        type: cli
        description: Antigravity CLI adapter
        models:
          - id: antigravity-default
            providerModel: default
        responseCommand:
          executable: antigravity
          args:
            - chat
            - --model
            - "{{provider_model}}"
            - --json
          input: request_json_stdin
          output: json_contract
          timeoutMs: 240000
        auth:
          loginCommand:
            executable: antigravity
            args:
              - auth
              - login
              - --no-browser
            timeoutMs: 900000
          statusCommand:
            executable: antigravity
            args:
              - auth
              - status
              - --json
            timeoutMs: 30000
      - id: codex-cli
        type: cli
        description: OpenAI (Codex CLI) adapter
        models:
          - id: codex-latest
            providerModel: codex-latest
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: openai-codex-latest
            providerModel: codex-latest
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: gpt-5-codex
            providerModel: gpt-5-codex
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: openai-gpt-5-codex
            providerModel: gpt-5-codex
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: openai-codex/chatgpt-5.3
            providerModel: codex-latest
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: openai-codex/chatgpt-5.2
            providerModel: codex-latest
            fallbackModels:
              - gemini-3-pro-preview
              - gemini-2.5-pro
          - id: gpt-4.1
            providerModel: gpt-4.1
            fallbackModels:
              - gemini-3-flash-preview
              - gemini-2.5-flash
          - id: openai-gpt-4.1
            providerModel: gpt-4.1
            fallbackModels:
              - gemini-3-flash-preview
              - gemini-2.5-flash
        responseCommand:
          executable: node
          args:
            - /app/dist/scripts/codex-appserver-bridge.js
            - --model
            - "{{provider_model}}"
          input: request_json_stdin
          output: json_contract
          timeoutMs: 240000
        auth:
          loginCommand:
            executable: sh
            args:
              - -lc
              - |
                if codex login --help 2>&1 | grep -q -- '--device-auth'; then
                  echo "Starting Codex device auth (no localhost callback required)..."
                  exec codex login --device-auth
                fi
                if codex login --help 2>&1 | grep -q -- '--no-browser'; then
                  echo "Starting Codex no-browser auth..."
                  exec codex login --no-browser
                fi
                echo "Starting Codex interactive login..."
                exec codex login
            timeoutMs: 900000
          statusCommand:
            executable: sh
            args:
              - -lc
              - |
                if codex login status >/tmp/codex-status.txt 2>&1; then
                  cat /tmp/codex-status.txt
                  exit 0
                fi
                if codex whoami >/tmp/codex-status.txt 2>&1; then
                  cat /tmp/codex-status.txt
                  exit 0
                fi
                cat /tmp/codex-status.txt
                exit 1
            timeoutMs: 30000
      - id: ollama-cli
        type: cli
        description: Ollama local backup adapter with native tool calling
        models:
          - id: ollama-qwen3-8b
            providerModel: qwen3:8b
          - id: ollama-qwen3-4b
            providerModel: qwen3:4b
        responseCommand:
          executable: sh
          args:
            - -lc
            - |
              REQUEST_JSON="$(cat)"
              OLLAMA_URL="${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}"
              OLLAMA_NUM_THREAD="${OLLAMA_NUM_THREAD:-10}"

              PAYLOAD="$(
                printf "%s" "$REQUEST_JSON" | node -e 'let d="";const numThread=Number(process.argv[1]);process.stdin.on("data",c=>d+=c).on("end",()=>{const req=JSON.parse(d);const msgs=Array.isArray(req.messages)?req.messages:[];const tools=Array.isArray(req.tools)?req.tools:[];const normalized=msgs.map((m)=>{const out={role:String(m.role||"user"),content:typeof m.content==="string"?m.content:JSON.stringify(m.content??"")};if(out.role==="tool"){const n=typeof m.name==="string"&&m.name?m.name:(typeof m.tool_call_id==="string"&&m.tool_call_id?m.tool_call_id:undefined);if(n)out.tool_name=n;}return out;});const body={model:"{{provider_model}}",messages:normalized,tools,stream:false};if(Number.isFinite(numThread)&&numThread>0){body.options={num_thread:Math.trunc(numThread)};}process.stdout.write(JSON.stringify(body));});' "$OLLAMA_NUM_THREAD"
              )"

              RESPONSE_JSON="$(curl -fsS "$OLLAMA_URL/api/chat" -H "Content-Type: application/json" -d "$PAYLOAD")" || exit $?

              printf "%s" "$RESPONSE_JSON" | node -e '
                let d="";
                process.stdin.on("data",c=>d+=c).on("end",()=>{
                  const res=JSON.parse(d);
                  const msg=res&&typeof res.message==="object"&&res.message?res.message:{};
                  const content=typeof msg.content==="string"?msg.content:"";

                  const normalizeToolCalls=(arr)=>{
                    const calls=Array.isArray(arr)?arr:[];
                    return calls.map((c,i)=>{
                      if(!c||typeof c!=="object") return null;
                      const fn=c.function&&typeof c.function==="object"?c.function:{};
                      const name=typeof c.name==="string"&&c.name
                        ?c.name
                        :(typeof fn.name==="string"&&fn.name?fn.name:"");
                      if(!name) return null;
                      const args=c.arguments!==undefined
                        ?c.arguments
                        :(fn.arguments!==undefined?fn.arguments:{});
                      return {
                        id:typeof c.id==="string"&&c.id?c.id:`call_${i+1}`,
                        name,
                        arguments:typeof args==="string"?args:JSON.stringify(args)
                      };
                    }).filter(Boolean);
                  };

                  const rawToolCalls=normalizeToolCalls(msg.tool_calls);
                  if(rawToolCalls.length>0){
                    process.stdout.write(JSON.stringify({
                      output_text:content,
                      tool_calls:rawToolCalls,
                      finish_reason:"tool_calls"
                    }));
                    return;
                  }

                  const parseContract=(input)=>{
                    const t=typeof input==="string"?input.trim():"";
                    if(!t) return null;
                    const seen=new Set();
                    const queue=[];
                    const push=(s)=>{
                      if(typeof s!=="string") return;
                      const v=s.trim();
                      if(!v||seen.has(v)) return;
                      seen.add(v);
                      queue.push(v);
                    };
                    const pushDerived=(s)=>{
                      const fence=/```(?:json)?\s*([\s\S]*?)```/gi;
                      let m;
                      while((m=fence.exec(s))!==null){
                        push(m[1]);
                      }
                      const start=s.indexOf("{");
                      const end=s.lastIndexOf("}");
                      if(start!==-1&&end>start){
                        push(s.slice(start,end+1));
                      }
                    };
                    const isContract=(j)=>j&&typeof j==="object"&&(
                      "output_text" in j || "tool_calls" in j || "finish_reason" in j || "text" in j || "content" in j
                    );

                    push(t);
                    pushDerived(t);

                    for(let i=0;i<queue.length&&i<50;i++){
                      const cur=queue[i];
                      pushDerived(cur);
                      let parsed=null;
                      try{parsed=JSON.parse(cur);}catch{}
                      if(!parsed||typeof parsed!=="object") continue;

                      if(isContract(parsed)){
                        const innerCandidates=[
                          typeof parsed.output_text==="string"?parsed.output_text:"",
                          typeof parsed.text==="string"?parsed.text:"",
                          typeof parsed.content==="string"?parsed.content:""
                        ].filter(Boolean);
                        for(const c of innerCandidates){
                          push(c);
                          pushDerived(c);
                          try{
                            const j2=JSON.parse(c);
                            if(isContract(j2)) return j2;
                          }catch{}
                        }
                        return parsed;
                      }

                      if(typeof parsed.response==="string") push(parsed.response);
                      if(parsed.message&&typeof parsed.message==="object"&&typeof parsed.message.content==="string"){
                        push(parsed.message.content);
                      }
                    }

                    return null;
                  };

                  const parsedContract=parseContract(content);
                  if(parsedContract){
                    const parsedToolCalls=normalizeToolCalls(parsedContract.tool_calls);
                    const outputText=typeof parsedContract.output_text==="string"
                      ?parsedContract.output_text
                      :(typeof parsedContract.text==="string"
                        ?parsedContract.text
                        :(typeof parsedContract.content==="string"?parsedContract.content:content));
                    const finishReason=parsedToolCalls.length>0
                      ?"tool_calls"
                      :(typeof parsedContract.finish_reason==="string"&&parsedContract.finish_reason
                        ?parsedContract.finish_reason
                        :"stop");
                    process.stdout.write(JSON.stringify({
                      output_text:outputText,
                      tool_calls:parsedToolCalls,
                      finish_reason:finishReason
                    }));
                    return;
                  }

                  const unfenced=content
                    .replace(/^```(?:json)?\s*/i,"")
                    .replace(/```$/,"")
                    .trim();
                  process.stdout.write(JSON.stringify({
                    output_text:unfenced||content,
                    finish_reason:"stop"
                  }));
                });
              '
          input: request_json_stdin
          output: json_contract
          timeoutMs: 240000
        auth:
          loginCommand:
            executable: sh
            args:
              - -lc
              - |
                echo "Ollama local provider does not require login."
                curl -fsS "${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}/api/tags" >/dev/null
            timeoutMs: 30000
          statusCommand:
            executable: sh
            args:
              - -lc
              - |
                curl -fsS "${OLLAMA_BASE_URL:-http://ollama.n8n-openai-gateway.svc.cluster.local:11434}/api/tags"
            timeoutMs: 30000
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: n8n-openai-cli-gateway-home
  namespace: n8n-openai-gateway
spec:
  # Set this if your cluster does not have a default storage class.
  # storageClassName: replace-with-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-models
  namespace: n8n-openai-gateway
spec:
  # Set this if your cluster does not have a default storage class.
  # storageClassName: replace-with-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: n8n-openai-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      nodeSelector:
        kubernetes.io/arch: arm64
      containers:
        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 11434
              name: http
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0:11434"
            - name: OLLAMA_CONTEXT_LENGTH
              value: "8192"
          volumeMounts:
            - name: ollama-models
              mountPath: /root/.ollama
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 20
          resources:
            requests:
              cpu: "2"
              memory: 8Gi
            limits:
              cpu: "10"
              memory: 20Gi
      volumes:
        - name: ollama-models
          persistentVolumeClaim:
            claimName: ollama-models
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: n8n-openai-gateway
  labels:
    app: ollama
spec:
  type: ClusterIP
  selector:
    app: ollama
  ports:
    - name: http
      port: 11434
      targetPort: 11434
      protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: n8n-openai-cli-gateway
  template:
    metadata:
      labels:
        app: n8n-openai-cli-gateway
    spec:
      serviceAccountName: n8n-openai-cli-gateway
      nodeSelector:
        kubernetes.io/arch: arm64
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        fsGroup: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: gateway
          image: ghcr.io/philly1084/n8n-openai-cli-gateway:main
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            capabilities:
              drop: ["ALL"]
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: PORT
              value: "8080"
            - name: HOST
              value: "0.0.0.0"
            - name: LOG_LEVEL
              value: "info"
            - name: MAX_JOB_LOG_LINES
              value: "500"
            - name: PROVIDERS_CONFIG_PATH
              value: /app/config/providers.yaml
            - name: HOME
              value: /var/lib/gateway-home
            - name: OLLAMA_BASE_URL
              value: "http://ollama.n8n-openai-gateway.svc.cluster.local:11434"
            - name: OLLAMA_NUM_THREAD
              value: "10"
            - name: CODEX_APPSERVER_MODEL_PROVIDER
              value: "openai"
            - name: CODEX_APPSERVER_TIMEOUT_MS
              value: "240000"
            - name: N8N_API_KEY
              valueFrom:
                secretKeyRef:
                  name: n8n-openai-cli-gateway-secrets
                  key: n8nApiKey
            - name: ADMIN_API_KEY
              valueFrom:
                secretKeyRef:
                  name: n8n-openai-cli-gateway-secrets
                  key: adminApiKey
          volumeMounts:
            - name: providers-config
              mountPath: /app/config/providers.yaml
              subPath: providers.yaml
            - name: provider-home
              mountPath: /var/lib/gateway-home
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 15
          startupProbe:
            httpGet:
              path: /healthz
              port: 8080
            failureThreshold: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: "1"
              memory: 1Gi
      volumes:
        - name: providers-config
          configMap:
            name: n8n-openai-cli-gateway-config
        - name: provider-home
          persistentVolumeClaim:
            claimName: n8n-openai-cli-gateway-home
---
apiVersion: v1
kind: Service
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
  labels:
    app: n8n-openai-cli-gateway
spec:
  type: ClusterIP
  selector:
    app: n8n-openai-cli-gateway
  ports:
    - name: http
      port: 80
      targetPort: 8080
      protocol: TCP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: n8n-openai-cli-gateway
  namespace: n8n-openai-gateway
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - gateway.example.com
      secretName: n8n-openai-cli-gateway-tls
  rules:
    - host: gateway.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: n8n-openai-cli-gateway
                port:
                  number: 80
